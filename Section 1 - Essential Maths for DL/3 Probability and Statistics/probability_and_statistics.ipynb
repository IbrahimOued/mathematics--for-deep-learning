{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability and statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the concepts in probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classical probability**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose we have a **random variable that maps the results of random experiments to the properties that interest us**. The aforementioned random variable **measures the likelihood (probability) of one or more sets of outcomes taking place**. We call this **the probability distribution**. Consider probability distribution as the foundation of the concepts we will study in this chapter.\n",
    "\n",
    "There are three ideas that are of great importance in probability theory—<font color=pink>probability space</font>, <font color=pink>random variables</font>, and <font color=pink>probability distribution</font>. Let's start by defining some of the more basic, yet important, concepts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sample space** is the **set of all the possible outcomes**. We denote this with $\\Omega$. Suppose we have $n$ likely outcomes—then, we have $\\omega_1, \\omega_2, \\dots, \\omega_n \\in \\Omega$ , where $w_i$ **is a possible outcome**. The subset of the sample space $(\\Omega)$ is called an **event**.\n",
    "\n",
    "Probability has a lot to do with sets, so let's go through some of the notation so that we can get a better grasp of the concepts and examples to come."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two events, $A$ and $B$, $\\subseteq \\Omega$. We have the following axioms:\n",
    "* The complement of $A$ is $A^C$, so $\\mathbb{P}(A^C) = 1 - \\mathbb{P}(A)$\n",
    "* If either $A$ or $B$ occurs, this is written $A \\cup B$ (read $A$ union $B$)\n",
    "* If both $A$ or $B$ occurs, this is written $A \\cap B$ (read $A$ intersect $B$)\n",
    "* If $A$ and $B$ are mutually exclusive (or disjoint), then we write $A \\cap B = \\emptyset$\n",
    "* If the occurence of $A$ implies the occurence of $B$, this is written as $A \\subseteq B$ (So, $\\mathbb{P}(A) \\leq \\mathbb{P}(B)$)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have an event, $A \\in \\Omega$, and $\\omega_1, \\omega_2, \\dots, \\omega_n \\in \\Omega$. In this case, the probability $A$ occuring is defined as follows:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(A) = \\frac{\\text{number of outcomes in A}}{\\text{number of outcomes in }\\Omega} = \\frac{|A|}{N}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the number of times $A$ can occur divided by the total number of possible outcomes in the sample space.\n",
    "\n",
    "Let's go through a simple example of flipping a coin. Here, **the sample space consists of all the possible outcomes of flipping the coin**. Say we are dealing with two coin tosses instead of one and $h$ means heads and $t$ means tails. So, the sample space is $\\Omega = {hh, ht, th, tt}$.\n",
    "\n",
    "All of the possible results of the experiment make up the event space. $\\mathcal{A}$ On finishing the experiment, we observe whether the outcome, $\\omega \\in \\Omega$, is in $A$.\n",
    "\n",
    "Since, in each event $A \\in \\mathcal{A}$, we denote $\\mathbb{P}(A)$ as the probability that the event will happen and we read $\\mathbb{P}(A)$ as the probability of $A$ occuring.\n",
    "\n",
    "Continuing on from the previous axioms. $\\mathbb{P}$ mus satisfy the following:\n",
    "* $0 \\leq \\mathbb{P} \\leq 1$ for all cases of $A \\in \\mathcal{A}$\n",
    "* $\\mathbb{P} = 1$\n",
    "* If the event $A_1, A_2, \\dots$ are disjoint and countably additive-that is, $A_i \\cap A_j \\neq \\emptyset$ for all cases of $i, j$-we then have $\\mathbb{P} \\left( \\bigcup_i A_i \\right) = \\sum_i \\mathbb{P}(A_i)$. The triplet $(\\Omega, \\mathcal{A}, \\mathbb{P})$ terms are known as **the probability space**.\n",
    "\n",
    "As a rule of thumb, when $\\mathbb{P}(A) = 1$, then event $A$ happens almost surely and when $\\mathbb{P}(A) = 0$, then event $A$ happens almost never.\n",
    "\n",
    "Using the preceding axioms we can derive the following:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(A) + \\mathbb{P}(A^C) = \\mathbb{P}(A \\cup A^C) = \\mathbb{P}(\\Omega) = 1\n",
    "$$\n",
    "\n",
    "So, $\\mathbb{P}(\\emptyset) = 0$.\n",
    "\n",
    "Additionally, if we have 2 events, $A$ and $B$, then we can deduce the following\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}A \\cap B.\n",
    "$$\n",
    "Continuing on from the preceding axioms,  must satisfy the following: $\\mathbb{P}$ must satisfy the following:\n",
    "\n",
    "$$\n",
    "0 \\leq \\mathbb{P} \\leq 1 \\text{ for all } A \\in \\mathcal{A}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sampling with or without replacement**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the probability of anything, we usually have to count things. Let's say we have a bucket filled with tennis balls and we pick a ball from the bucket $r$ times; so, there are $n_1$ possibilities for the first pick, $n_2$ for the next pick, and so on. The total number of choices ends up being $n_1 \\times n_2 \\times \\dots \\times n_r$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now assume that there is a total of $n$ items in the bucket and we must pick $r$ of them. Then, let $R = {1, 2, \\dots, r}$ be the list of items picked and let $N = {1, 2, \\dots, n}$ be the total number of items. This can be written as a function, as follows:\n",
    "\n",
    "$$\n",
    "f: R \\to N\n",
    "$$\n",
    "\n",
    "Here, $f(i)$ is the $i^{th}$ item."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling with replacement is when we pick an item at random and then put it back so that the item can be picked again.\n",
    "\n",
    "However, sampling without replacement refers to when we choose an item and don't put it back, so we cannot pick it again. Let's see an example of both.\n",
    "\n",
    "Say we need to open the door to our office and we have a bag containing $n$ keys; they all look identical, so there's no way of differentiating between them. \n",
    "\n",
    "The first time we try picking a key, we replace each one after trying it, and we manage to find the correct key on the $r^{th}$ trial, implying we got it wrong $r-1$ times. The probability is then as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{(n-1)(n-1)\\dots(n-1)(1)}{n^r} = \\frac{(n-1)^{r-1}}{n^r}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know that our earlier strategy wasn't the smartest, so this time we try it again but without replacement and eliminate each key that doesn't work. Now, the probability is as follows:\n",
    "\n",
    "$$\n",
    "\\frac{(n-1)(n-2)\\dots(n-r+1)(1)}{n(n-1)\\dots (n-r+1)} = \\frac{1}{n}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multinomial coefficient**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from the binomial theorem (which you likely learned in high school). That the following is true:\n",
    "\n",
    "$$\n",
    "(x+y)^n = x^n + \\begin{pmatrix}n \\\\ 1 \\end{pmatrix} x^{n + 1}y +  \\begin{pmatrix}n \\\\ 2 \\end{pmatrix}x^{n-2}y^2 + \\dots + y^n\n",
    "$$\n",
    "\n",
    "Then, the trinomial is as follows:\n",
    "\n",
    "$$\n",
    "(a + y + z)^n = \\sum_{n_1, n_2, n_3} \\begin{pmatrix} n \\\\ n_1, n_2, n_3 \\end{pmatrix} n^{n_1}y^{n_2}{z^{n_3}}\n",
    "$$\n",
    "\n",
    "Say we have $n$ pieces of candy and there are blue- and red-colored candies. The different ways that that we can pick the candies is defined as $$\\begin{pmatrix} n \\\\ k \\end{pmatrix}$$, which is read as $n$ choose $k$.\n",
    "\n",
    "The multinomial coefficient is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} n \\\\ n_1, n_2, n_3 \\end{pmatrix} = \\begin{pmatrix} n \\\\ n_1\\end{pmatrix} \\begin{pmatrix} n-n_1 \\\\ n_2\\end{pmatrix} \\dots \\begin{pmatrix} n-n_1-\\dots - n_{k-1} \\\\ n_k\\end{pmatrix} = \\frac{n!}{n_1!n_2!\\dots n_k!}\n",
    "$$\n",
    "\n",
    "This way, we spread $n$ items over $k$ positions, where the $i^{th}$ position has $n_i$ items.\n",
    "\n",
    "For example, say we're playing cards and we have four players. A deck of cards has $52$ cards and we deal $13$ cards to each player. So, the number of possible ways that we can distribute the cards is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} 52 \\\\ 13, 13, 13, 13\\end{pmatrix} = \\frac{52!}{(13!)^4} = 5.36 \\times 10^{28}\n",
    "$$\n",
    "\n",
    "This is absolutely massive!\n",
    "\n",
    "This is where **Stirling's formula** comes to the rescue. It allows us to approximate the answer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stirlings's formula**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of argument, let's say $log n! \\sim n log n$.\n",
    "\n",
    "We know that the following is true:\n",
    "\n",
    "$$\n",
    "\\int^n_1 log x dx \\leq \\sum^n_1 log k \\leq \\int^{n+1}_1 log x dx\n",
    "$$\n",
    "\n",
    "![](stirling_formula.png)\n",
    "\n",
    "Now, by evaluating the integral, we get the following:\n",
    "\n",
    "$$\n",
    "n log n - n + 1 \\leq log n ! \\leq (n+1)log(n+1) - n\n",
    "$$\n",
    "\n",
    "We know divide both sides by $nlogn$ and take the limit as $n \\to \\infty$. We observe that both sides tend to be $1$, So we have the following:\n",
    "\n",
    "$$\n",
    "log \\frac{n!e^n}{n^{n+\\frac{1}{2}}} = log\\sqrt{2\\pi} + \\Omicron(\\frac{1}{n})\n",
    "$$\n",
    "\n",
    "Furthermore, we have the following:\n",
    "\n",
    "$$\n",
    "n! \\sim \\sqrt{2\\pi}n^{n+\\frac{1}{2}}e^{-n}\n",
    "$$\n",
    "> We will avoid looking into the proof for Sterling's formula, but if you're interested in learning more, then I highly recommend looking it up."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Independance**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Events are independent when they are not related to each other; that is, the outcome of one has no bearing on the outcome of another.\n",
    "\n",
    "Suppose we have two independent events, $A$ and $B$. Then, we can test the following:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{B}\n",
    "$$\n",
    "\n",
    "If this is not true, then the events are dependant.\n",
    "\n",
    "Imagine you're at a casino and you're playing craps. You throw two dice—their outcomes are independent of each other. \n",
    "\n",
    "An interesting property of independence is that if A and B are independent events, then so are A and $B^C$. \n",
    "\n",
    "Let's take a look and see how this works:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}(A \\cap B^C) & = \\mathbb{P}(A) - \\mathbb{P}(A \\cap B) \\\\\n",
    "& = \\mathbb{P}(A) - \\mathbb{P}(A)\\mathbb{P}(B)\\\\\n",
    "& = \\mathbb{P}(A)(1 - \\mathbb{P}(B)) \\\\\n",
    "& = \\mathbb{P}(A)\\mathbb{P}(B^C)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have multiple events, $A_1$, $A_2$, $\\dots$, $A_n$ we call them mutually independant when $\\mathbb{P}(A_1 \\cap A_2 \\cap \\dots \\cap A_n) = \\mathbb{P}(A_1) \\mathbb{P}(A_2) \\dots \\mathbb{P}(A_n)$ for all cases of $n \\geq 2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose we conduct two experiments in a lab, we model them independently as $\\Omega_1 = {\\alpha_1, \\alpha_2, \\dots}$ and $\\Omega_2 = {\\beta_1, \\beta_2, \\dots}$ and the probabilities of each are $\\mathbb{P}(\\alpha_i) = p_i$ and $\\mathbb{P}(\\beta_i) = q_i$, respectively. If the two are independant, then we have the following\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\alpha_i \\beta_i) = p_i q_i\n",
    "$$\n",
    "\n",
    "This is far all cases of $i$ and $j$, and or new sample space is $\\Omega = \\Omega_1 \\times \\Omega_2$\n",
    "\n",
    "Now say $A$ and $B$ are events on the $\\Omega_1$ and $\\Omega_2$ experiments, respectively. We can view them as subspaces of the new sample space, $\\Omega$, by calculating $A \\times \\Omega_2$ and $B \\times \\Omega_1$, which leads to the following:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(A \\cap B) = \\sum_{\\alpha_i \\in A, \\beta_i \\in B}p_iq_i = \\sum_{a_i \\in A}p_i \\sum_{\\beta_i \\in B}q_i = \\mathbb{P}(A)\\mathbb{P}(B)\n",
    "$$\n",
    "\n",
    "Even though we normally define independence as different (unrelated) results in the same experiment, we can extend this to an arbitrary number of independent experiments as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Discrete distributions**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete refers to when our sample space is countable, such as in the cases of coin tosses or rolling dice.\n",
    "In discrete probability distributions, the sample space is $\\Omega = \\{\\omega_1, \\omega_2, \\dots, \\omega_n\\}$ and $p_i = \\mathbb{P}(\\{\\omega_i\\})$\n",
    "\n",
    "The following are the $6$ different kinds of discrete distributions that we often encounter in probability theory:\n",
    "* Bernoulli distribution\n",
    "* Binomial distribution\n",
    "* Geometric distribution\n",
    "* Hypergeometric distribution\n",
    "* Poisson distribution\n",
    "\n",
    "Let's define them in order\n",
    "\n",
    "For the Bernoulli distribution, let's use the example of a coin toss, where our sample space is $\\Omega = {H, T}$ and $p \\in [0, 1]$ (that $0 \\leq p \\leq 1$). We denote the distribution as $B(1, p)$ such that the following applies:\n",
    "$$\n",
    "\\mathbb{P}(H) = p and \\mathbb{P}(T) = 1 - p\n",
    "$$\n",
    "\n",
    "But now, let's suppose the coin is flipped $n$ times, each with the aforementioned probability of $p$ for the outcome being heads. Then the binomial distribution, denoted as $B(n,p)$, states the following:\n",
    "$$\n",
    "\\mathbb{P}(THHTT \\dots H) = (1-p)pp(1-p)(1-p) \\dots p\n",
    "$$\n",
    "\n",
    "Therefore, we have the following:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\text{four heads}) = \\left(\\frac{n}{4}\\right)p^4(1-p)^{n-4}\n",
    "$$\n",
    "\n",
    "Generally, the binomial distribution is written as follows:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\text{k heads}) = \\left(\\frac{n}{k}\\right)ip^k(1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "The **geometric distribution does not keep any memory of past events and so is memory-less**. Suppose we flip our coin again; this distribution does not give us any indication as to when we can expect a heads result or how long it will take. So, we write the probability of getting heads after getting tails k times as follows:\n",
    "\n",
    "$$\n",
    "p_k = (1-p)^kp\n",
    "$$\n",
    "\n",
    "Let's say we have a bucket filled with balls of two colors—red and black (which we will denote as $r$ and $b$, respectively). From the bucket, we have picked out n balls and we want to figure out the probability that k of the balls are black. For this, we use the hypergeometric distribution, which looks as follows:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\text{k black}) = \\frac{\\left(\\frac{b}{k}\\right)\\frac{r}{n-k}}{\\left(\\frac{b+r}{n}\\right)}\n",
    "$$\n",
    "\n",
    "The Poisson distribution is a bit different from the other distributions. It is used to model rare events that occur at a rate, $\\lambda$. It is denoted as $p(\\lambda)$ and is written as follows:\n",
    "\n",
    "$$\n",
    "p_k = \\frac{\\lambda^k}{k!}e^{-\\lambda}\n",
    "$$\n",
    "\n",
    "This is true for all cases of $k \\in \\N$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conditional probability**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional probabilities are useful when the occurrence of one event leads to the occurrence of another. If we have two events, $A$ and $B$, where $B$ has occurred and we want to find the probability of $A$ occurring, we write this as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
