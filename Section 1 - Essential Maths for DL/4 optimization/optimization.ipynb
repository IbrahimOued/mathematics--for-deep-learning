{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization is a branch of applied mathematics that has applications in a multitude of fields, such as physics, engineering, economics, and so on, and is of vital importance in developing and training of deep neural networks. In this chapter, a lot of what we covered in previous chapters will be very relevant, particularly linear algebra and calculus.\n",
    "\n",
    "As we know, deep neural networks are developed on computers and are, therefore, expressed mathematically. More often than not, training deep learning models comes down to finding the correct (or as close to the correct) set of parameters. We will learn more about this as we progress further through this book.\n",
    "\n",
    "In this chapter, we'll mainly learn about two types of continuous optimization—constrained and unconstrained. However, we will also briefly touch on other forms of optimization, such as genetic algorithms, particle swarm optimization, and simulated annealing. Along the way, we will also learn when and how to use each of these techniques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding optimization and it's different types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In optimization, our goal is to either minimize or maximize a function. For example, a business wants to minimize its costs while maximizing its profits or a shooper might want to get as much as possible while spending as little as possible. Therefore, the goal of optimization is to find the best case of , which is denoted by x* (where x is a set of points), that satisfies certain criteria. These criteria are, for our purposes, mathematical functions known as **objective functions**.\n",
    "\n",
    "For example, let's suppose we have the $f(x) = x^4 + 8x^3 + 10x^2 - 14x - 4$ equation. If we plot it, we get the following graph:\n",
    "\n",
    "![Alt text](plot.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will recall from Chapter 1, Vector Calculus, that **we can find the gradient of a function by taking its derivative, equating it to $0$, and solving for $x$**. We can find the point(s) at which the function has a minimum or maximum, as follows:\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = 4x^3 + 24x^2 + 20x - 14\n",
    "$$\n",
    "\n",
    "After solving this equation, we find that it has three distinct solutions (that is, three points where the minima and maxima occur). \n",
    "\n",
    "To find which of these three solutions are the minima and maxima, we find the second derivative, $\\frac{d^2f}{dx^2} = 12x^2 + 48x + 20$, and check whether our stationary points are positive or negative.\n",
    "\n",
    "Visually, when we see the graph, we can identify the local and global minima, but it isn't as simple as this when we calculate it computationally. So, instead, we start at a value and follow the gradient until we get to the minima (hopefully, the global minima). \n",
    "\n",
    "Say we start from the right side at $x = 2$. The gradient is negative, which means we move to the left incrementally (these increments are called **step size**) and we get to the local minima, which isn't the one we want to find. However, if we start at $x = -2$, then we end up at the global minima. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constrained optimization, in general, has certain rules or constraints attached that must be followed. In general, the problem is defined in the following form:\n",
    "\n",
    "$$\n",
    "\\text{minimize }f(x) \\text{ subject to }h(x) = b\\text{ given that }x \\in X\n",
    "$$\n",
    "\n",
    "In the preceding equation, $x \\in \\R^n$ contains the decision variables, $f : \\R^n \\to \\R$ is our objective function, $h : \\R^n \\to R^m$ and $b \\in R^m$ are the functional constraints, while $X \\subseteq \\R^n$ is the regional constraint.\n",
    "\n",
    "> All of these variables are vectors; in fact, all of the variables in this chapter will be vectors, so for simplification, we will not be writing them in boldface as we did previously, in Chapter 1, Vector Calculus, and Chapter 2, Linear Algebra.\n",
    "\n",
    "Sometimes, our constraints could be in the form of an inequality, such as $h(x) \\geq b$, and we can add in a slack variable, $z$, which now makes our functional constraint $h(x) - z = b$ and the regional constraint $z \\geq 0$.\n",
    "\n",
    "We could simply write out all the constraints explicitly, but that's just too messy. We generally write them as follows:\n",
    "\n",
    "$$\n",
    "\\text{minimize }c^Tx \\text{ subject to }A(x) \\geq b\\text{, where }x \\geq 0\n",
    "$$\n",
    "\n",
    "This is the general form of a linear program. The standard form, however, is written as follows:\n",
    "\n",
    "$$\n",
    "\\text{minimize }c^Tx \\text{ subject to }A(x) = b\\text{, where }x \\geq 0\n",
    "$$\n",
    "\n",
    "I know this may all seem very unclear right now, but don't fear—we will make sense of all of it soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
